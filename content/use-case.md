## Use case: describing a Linked Data Fragments experiment
{:#use-case}

In this section, we will provide a semantic description
of the experiment performed in [a previous ISWC2014 article](cite:citesAsAuthority verborgh_iswc_2014),
which involves specific software configurations
of a Linked Data Fragments (LDF) client and server.
This description serves as a guiding example
on how to create such descriptions for other evaluations.
The intention is that future research articles
directly describe their experimental setup this way,
either through HTML with embedded RDFa
or as a reference to an IRI of an RDF document.

For this use case,
we have semantically described the [LDF server](https://github.com/LinkedDataFragments/Server.js/tree/feature-lsd) module and its 32 components.
Instead of the former domain-specific [JSON configuration file](https://github.com/LinkedDataFragments/Server.js/blob/master/config/config-example.json),
the [semantic configuration](https://github.com/LinkedDataFragments/Server.js/blob/feature-lsd/config/config-example.json) is Linked Data
and can be instantiated automatically by Components.js.

The ISWC2014 LDF experiment can be described by the following steps:

0. 1 virtual machine for the server was created.
0. 1 virtual machine for a cache was created.
0. 60 virtual machines for clients were created.
0. The generated [BSBM dataset](#) was copied to the server.
0. [The server software configuration](#),
   implementing the [TPF specification](https://www.hydra-cg.com/spec/latest/triple-pattern-fragments/),
   was installed on the server.
0. [The client software configuration](#),
   implementing the [SPARQL 1.1 protocol](https://www.w3.org/TR/sparql11-protocol/),
   was installed on each client.
0. Each client machine executed four processes of the [BSBM benchmark](#)
   with the client software.
0. We recorded the CPU time, RAM usage of each client,
   the CPU time and RAM usage of the server,
   and we measured the ingoing and outgoing bandwidth of the cache.
0. We published the [results](http://data.linkeddatafragments.org/benchmark) online.

{:.todo}
Sarven and Ruben V.:
The above description should become fully semantified.
We can do this in the article itself in HTML+RDFa
and/or as a separate Turtle file.
(Perhaps the Turtle file can be autogenerated.)

Note how in the description of the experiment above,
we refer to the exact software configurations by their IRI,
as well as to the specifications they implement.
Based on these IRIs,
other researchers can immediately instantiate the same configuration,
or derive their own similar configurations
to create comparative experiments.
While software container solutions, such as [Docker](https://www.docker.com/),
could also provide immediate instantiation,
their configuration is on a much higher level.
Instead, the Object-Oriented Components ontology
captures the low-level wiring between components,
enabling researchers to swap individual algorithms
or component settings.

For example, based on the above description,
the exact same experiment can be performed
with [different client-side algorithms](cito:obtainsBackgroundFrom vanherwegen_eswc_2015)
or [different server-side interfaces](cito:obtainsBackgroundFrom hartig_odbase_2016).
A common practice to achieve this currently,
as done in the aforementioned works [](cito:citesAsEvidence vanherwegen_eswc_2015,hartig_odbase_2016),
is to implement modifications in separate code repository branches.
Unfortunately,
such branches typically diverge from the main code tree,
and hence cannot be easily evaluated afterwards
with later versions of other components.
By implementing them as independent modules instead,
they can be plugged in automatically
by minimally altering the declarative experiment description.
